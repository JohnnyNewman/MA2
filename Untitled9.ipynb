{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('MA2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "03cbb39f8eea8d45c13ef9784b5cce53285fc6d097512ff599f06e2d2e50c7a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_dataset = pickle.load(open(\"save/T007_Transformer/train_dataset.p\", \"rb\"))\n",
    "test_dataset = pickle.load(open(\"save/T007_Transformer/test_dataset.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "#from torch import nn\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from https://stackoverflow.com/questions/61616810/how-to-do-cubic-spline-interpolation-and-integration-in-pytorch\n",
    "\n",
    "#import torch\n",
    "\n",
    "def h_poly(t):\n",
    "    tt = t[None, :]**torch.arange(4, device=t.device)[:, None]\n",
    "    A = torch.tensor([\n",
    "        [1, 0, -3, 2],\n",
    "        [0, 1, -2, 1],\n",
    "        [0, 0, 3, -2],\n",
    "        [0, 0, -1, 1]\n",
    "    ], dtype=t.dtype, device=t.device)\n",
    "    return A @ tt\n",
    "\n",
    "\n",
    "def interp(x, y, xs):\n",
    "    m = (y[1:] - y[:-1]) / (x[1:] - x[:-1])\n",
    "    m = torch.cat([m[[0]], (m[1:] + m[:-1]) / 2, m[[-1]]])\n",
    "    idxs = torch.searchsorted(x[1:], xs)\n",
    "    dx = (x[idxs + 1] - x[idxs])\n",
    "    hh = h_poly((xs - x[idxs]) / dx)\n",
    "    return hh[0] * y[idxs] + hh[1] * m[idxs] * dx + hh[2] * y[idxs + 1] + hh[3] * m[idxs + 1] * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirfoilModel(LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = 128\n",
    "        #self.hparams.batch_size = 64\n",
    "        self.lr=1e-3\n",
    "        \n",
    "        self.train_variance = 1.0\n",
    "        \n",
    "        c1 = 16\n",
    "        c2 = 8\n",
    "        k2 = 8\n",
    "        c3 = 32\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_0 = nn.Conv1d(in_channels=6,out_channels=c1,kernel_size=5,stride=1, padding=2)\n",
    "\n",
    "        self.layer_11 = nn.Conv1d(in_channels=c1,out_channels=c2,kernel_size=3,stride=1, padding=1)\n",
    "        self.layer_12 = nn.Conv1d(in_channels=c2,out_channels=c2,kernel_size=3,stride=1, padding=1)\n",
    "        self.layer_13 = nn.Conv1d(in_channels=c2,out_channels=c2,kernel_size=3,stride=1, padding=1)\n",
    "        self.layer_14 = nn.Conv1d(in_channels=c2,out_channels=c2,kernel_size=3,stride=1, padding=1)\n",
    "        self.relu_11 = nn.ReLU()\n",
    "        self.relu_12 = nn.ReLU()\n",
    "        self.relu_13 = nn.ReLU()\n",
    "        self.relu_14 = nn.ReLU()\n",
    "\n",
    "        self.layer_2 = nn.Conv1d(in_channels=8*4,out_channels=c3,kernel_size=5,stride=1, padding=2)\n",
    "        self.relu_2 = nn.ReLU()\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=32, nhead=8, dim_feedforward=64, dropout=0.001)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=3)\n",
    "        \n",
    "        self.output = nn.Conv1d(in_channels=c3,out_channels=11,kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        self.beta_dist = torch.distributions.beta.Beta(2,2)\n",
    "        \n",
    "        self.ma_aoa_lin1 = nn.Linear(2,16)\n",
    "        self.ma_aoa_relu1 = nn.ReLU()\n",
    "        self.ma_aoa_lin2 = nn.Linear(16,32)\n",
    "        self.ma_aoa_relu2 = nn.ReLU()\n",
    "        \n",
    "        latenc_layers = nn.TransformerDecoderLayer(d_model=32, nhead=8, dim_feedforward=64, dropout=0.001)\n",
    "        self.latenc_transformer = nn.TransformerDecoder(latenc_layers, num_layers=2)        \n",
    "        \n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model=32, nhead=8, dim_feedforward=64, dropout=0.001)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers=3)\n",
    "        \n",
    "    def data_augmentation2(self, x):\n",
    "        \n",
    "        s = x[:,1]\n",
    "        ds = s[:,1:] - s[:,:-1]\n",
    "        \n",
    "        rv = self.beta_dist.rsample([s.shape[0], s.shape[1]-2]).to(self.device)\n",
    "        snew = s.clone()\n",
    "        #print(snew.device, rv.device, ds.device)\n",
    "        snew[:,1:-1] += 0.5 * ( (rv > 0)*ds[:,1:] - (rv < 0)* ds[:,:-1] )\n",
    "\n",
    "        #xnew = interp(s, x, snew)\n",
    "        \n",
    "        xnew = torch.zeros_like(x, device=self.device)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                xnew[i,j] = interp(s[i], x[i,j], snew[i])\n",
    "        \n",
    "        indices = torch.rand(xnew.shape[2]) >= 0.25\n",
    "        xnew = xnew[:,:,indices]\n",
    "        \n",
    "        return xnew\n",
    "\n",
    "    def data_augmentation(self, x):\n",
    "\n",
    "        xnew = x.clone()\n",
    "        indices = torch.rand(xnew.shape[2]) >= 0.1\n",
    "        xnew = xnew[:,:,indices]\n",
    "        \n",
    "        return xnew\n",
    "    \n",
    "    def augment_output(self, x, y):\n",
    "\n",
    "        xnew = x.clone()\n",
    "        ynew = y.clone()\n",
    "        indices = torch.rand(xnew.shape[2]) >= 0.1\n",
    "        xnew = xnew[:,:,indices]\n",
    "        ynew = ynew[:,:,indices]\n",
    "\n",
    "        return xnew, ynew\n",
    "\n",
    "\n",
    "    def augment_output2(self, x, y):\n",
    "        \n",
    "        s = x[:,1]\n",
    "        ds = s[:,1:] - s[:,:-1]\n",
    "        \n",
    "        rv = self.beta_dist.rsample([s.shape[0], s.shape[1]-2]).to(self.device)\n",
    "        snew = s.clone()\n",
    "        #print(snew.device, rv.device, ds.device)\n",
    "        snew[:,1:-1] += 0.5 * ( (rv > 0)*ds[:,1:] - (rv < 0)* ds[:,:-1] )\n",
    "\n",
    "        #xnew = interp(s, x, snew)\n",
    "        \n",
    "        xnew = torch.zeros_like(x, device=self.device)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                xnew[i,j] = interp(s[i], x[i,j], snew[i])\n",
    "                \n",
    "        ynew = torch.zeros_like(y, device=self.device)\n",
    "        for i in range(y.shape[0]):\n",
    "            for j in range(y.shape[1]):\n",
    "                ynew[i,j] = interp(s[i], y[i,j], snew[i])\n",
    "        \n",
    "        indices = torch.rand(xnew.shape[2]) >= 0.25\n",
    "        xnew = xnew[:,:,indices]\n",
    "        ynew = ynew[:,:,indices]\n",
    "        \n",
    "        return xnew, ynew\n",
    "        \n",
    "    def airfoil_encoder(self, x):\n",
    "        \n",
    "        #xxx = torch.cat((x, x, x), dim=2)\n",
    "        #x_0 = self.layer_0(xxx)\n",
    "        x_0 = self.layer_0(x)\n",
    "\n",
    "        x_11 = self.relu_11(self.layer_11(x_0))\n",
    "        x_12 = self.relu_12(self.layer_12(x_11))\n",
    "        x_13 = self.relu_13(self.layer_13(x_12))\n",
    "        x_14 = self.relu_14(self.layer_14(x_13))\n",
    "\n",
    "        x_1 = torch.cat((x_11, x_12, x_13, x_14), dim=1)\n",
    "        \n",
    "        x_2 = self.relu_2(self.layer_2(x_1))\n",
    "        \n",
    "        ### Insert Transformer!\n",
    "        \n",
    "        x_3 = x_2\n",
    "        \n",
    "        return x_3\n",
    "        \n",
    "    def latent_encoder(self, x_3, ma, aoa):\n",
    "        \n",
    "        #ma = torch.ones(x_3.shape[0], 1,193).to(self.device)*ma.view(-1, 1, 1)\n",
    "        #aoa = torch.ones(x_3.shape[0], 1, 193).to(self.device)*aoa.view(-1, 1, 1)\n",
    "        \n",
    "        #print(\"lat\", x_3.device, ma.device, aoa.device)\n",
    "        \n",
    "        ma_aoa = torch.stack((ma,aoa)).to(self.device)\n",
    "        ma_aoa1 = self.ma_aoa_relu1(self.ma_aoa_lin1(ma_aoa.T))\n",
    "        ma_aoa2 = self.ma_aoa_relu2(self.ma_aoa_lin2(ma_aoa1)).view(-1,1,32)\n",
    "        \n",
    "        #relu(lin(cat((ma,aoa))))\n",
    "        #x_4 = torch.cat((x_3, aoa, ma), dim=1)        \n",
    "        \n",
    "        ### Insert Transformer!\n",
    "        \n",
    "        #z = self.transformer_encoder(x_4.transpose(1,2))\n",
    "        \n",
    "        #print(ma_aoa2.shape, x_3.shape)\n",
    "        #print(ma_aoa2.device, x_3.device)\n",
    "        \n",
    "        z = self.latenc_transformer(ma_aoa2, x_3.transpose(1,2))\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    \n",
    "    def airfoil_predictor(self, z, x_output):\n",
    "        \n",
    "        ### Insert Transformer!\n",
    "        \n",
    "        #y = self.output(z.transpose(1,2))      \n",
    "        \n",
    "        \n",
    "        #x3_output = self.airfoil_encoder(x_output)\n",
    "        \n",
    "        #print(z.shape, x_output.shape, x3_output.shape)\n",
    "        \n",
    "        #y = self.transformer_decoder(x3_output, z.transpose(1,2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        x3_output = self.airfoil_encoder(x_output).transpose(1,2).transpose(0,1)\n",
    "        z = z.transpose(0,1)\n",
    "        \n",
    "        #print(z.shape, x_output.shape, x3_output.shape)\n",
    "\n",
    "        y = self.transformer_decoder(x3_output, z).transpose(0,1).transpose(1,2)\n",
    "        \n",
    "        out = self.output(y)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward(self, x, ma, aoa):\n",
    "        \n",
    "        xnew = self.data_augmentation(x)\n",
    "        x3 = self.airfoil_encoder(xnew)\n",
    "        z = self.latent_encoder(x3, ma, aoa)\n",
    "        y = self.airfoil_predictor(z)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        qoip, ma, _, aoa = batch\n",
    "        \n",
    "        #print(qoip.device, ma.device, aoa.device)\n",
    "        \n",
    "        x = qoip[:,:6,:]\n",
    "        y_target = qoip\n",
    "        \n",
    "        xout, y_target = self.augment_output(x.clone(), y_target.clone())\n",
    "        \n",
    "        xnew = self.data_augmentation(x.clone())\n",
    "        x3 = self.airfoil_encoder(xnew)\n",
    "        z = self.latent_encoder(x3, ma, aoa)\n",
    "        #y = self.airfoil_predictor(z, self.data_augmentation(x))\n",
    "        y = self.airfoil_predictor(z, xout)\n",
    "        \n",
    "        #y[:,5,:] = 10. * y[:,5,:]\n",
    "        #y_target[:,5,:] = 10. * y_target[:,5,:]\n",
    "        \n",
    "        \n",
    "        loss1 = F.mse_loss(y, y_target) * 1e6\n",
    "        \n",
    "        loss = loss1\n",
    "        \n",
    "        self.log('train_loss', loss1)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        qoip, ma, _, aoa = batch\n",
    "        x = qoip[:,:6,:]\n",
    "        y_target = qoip\n",
    "        \n",
    "        xout, y_target = self.augment_output(x.clone(), y_target.clone())\n",
    "        \n",
    "        xnew = self.data_augmentation(x.clone())\n",
    "        x3 = self.airfoil_encoder(xnew)\n",
    "        z = self.latent_encoder(x3, ma, aoa)\n",
    "        #y = self.airfoil_predictor(z, self.data_augmentation(x))\n",
    "        y = self.airfoil_predictor(z, xout)\n",
    "        \n",
    "        #y[:,5,:] *= 10.\n",
    "        #y_target[:,5,:] *= 10.\n",
    "        \n",
    "        loss1 = F.mse_loss(y, y_target) * 1e6\n",
    "        \n",
    "        loss = loss1\n",
    "        \n",
    "        self.log('val_loss', loss1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2000, eta_min=0.0001)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        #print(\"get dataloader \", self.batch_size)\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True, pin_memory=True)    \n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(test_dataset, batch_size=min(self.batch_size,len(test_dataset)), num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AirfoilModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = Trainer(gpus=1, weights_summary='full', precision=16, check_val_every_n_epoch=2, max_epochs=5_000,\n",
    "                 limit_train_batches=1.0, auto_lr_find=False, callbacks=[lr_monitor]) #, auto_scale_batch_size=None\n",
    "#train_loader = DataLoader(train_dataset, batch_size=1468, shuffle=True, pin_memory=True)\n",
    "#val_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, pin_memory=True)\n",
    "#trainer.fit(model, train_loader)\n",
    "#trainer.tune(model)\n",
    "model.batch_size = 32\n",
    "model.train_variance = 0.01 #1.0\n",
    "model.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "    | Name                                                 | Type                    | Params\n",
      "---------------------------------------------------------------------------------------------------\n",
      "0   | layer_0                                              | Conv1d                  | 496   \n",
      "1   | layer_11                                             | Conv1d                  | 392   \n",
      "2   | layer_12                                             | Conv1d                  | 200   \n",
      "3   | layer_13                                             | Conv1d                  | 200   \n",
      "4   | layer_14                                             | Conv1d                  | 200   \n",
      "5   | relu_11                                              | ReLU                    | 0     \n",
      "6   | relu_12                                              | ReLU                    | 0     \n",
      "7   | relu_13                                              | ReLU                    | 0     \n",
      "8   | relu_14                                              | ReLU                    | 0     \n",
      "9   | layer_2                                              | Conv1d                  | 5.2 K \n",
      "10  | relu_2                                               | ReLU                    | 0     \n",
      "11  | transformer_encoder                                  | TransformerEncoder      | 25.6 K\n",
      "12  | transformer_encoder.layers                           | ModuleList              | 25.6 K\n",
      "13  | transformer_encoder.layers.0                         | TransformerEncoderLayer | 8.5 K \n",
      "14  | transformer_encoder.layers.0.self_attn               | MultiheadAttention      | 4.2 K \n",
      "15  | transformer_encoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 1.1 K \n",
      "16  | transformer_encoder.layers.0.linear1                 | Linear                  | 2.1 K \n",
      "17  | transformer_encoder.layers.0.dropout                 | Dropout                 | 0     \n",
      "18  | transformer_encoder.layers.0.linear2                 | Linear                  | 2.1 K \n",
      "19  | transformer_encoder.layers.0.norm1                   | LayerNorm               | 64    \n",
      "20  | transformer_encoder.layers.0.norm2                   | LayerNorm               | 64    \n",
      "21  | transformer_encoder.layers.0.dropout1                | Dropout                 | 0     \n",
      "22  | transformer_encoder.layers.0.dropout2                | Dropout                 | 0     \n",
      "23  | transformer_encoder.layers.1                         | TransformerEncoderLayer | 8.5 K \n",
      "24  | transformer_encoder.layers.1.self_attn               | MultiheadAttention      | 4.2 K \n",
      "25  | transformer_encoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 1.1 K \n",
      "26  | transformer_encoder.layers.1.linear1                 | Linear                  | 2.1 K \n",
      "27  | transformer_encoder.layers.1.dropout                 | Dropout                 | 0     \n",
      "28  | transformer_encoder.layers.1.linear2                 | Linear                  | 2.1 K \n",
      "29  | transformer_encoder.layers.1.norm1                   | LayerNorm               | 64    \n",
      "30  | transformer_encoder.layers.1.norm2                   | LayerNorm               | 64    \n",
      "31  | transformer_encoder.layers.1.dropout1                | Dropout                 | 0     \n",
      "32  | transformer_encoder.layers.1.dropout2                | Dropout                 | 0     \n",
      "33  | transformer_encoder.layers.2                         | TransformerEncoderLayer | 8.5 K \n",
      "34  | transformer_encoder.layers.2.self_attn               | MultiheadAttention      | 4.2 K \n",
      "35  | transformer_encoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 1.1 K \n",
      "36  | transformer_encoder.layers.2.linear1                 | Linear                  | 2.1 K \n",
      "37  | transformer_encoder.layers.2.dropout                 | Dropout                 | 0     \n",
      "38  | transformer_encoder.layers.2.linear2                 | Linear                  | 2.1 K \n",
      "39  | transformer_encoder.layers.2.norm1                   | LayerNorm               | 64    \n",
      "40  | transformer_encoder.layers.2.norm2                   | LayerNorm               | 64    \n",
      "41  | transformer_encoder.layers.2.dropout1                | Dropout                 | 0     \n",
      "42  | transformer_encoder.layers.2.dropout2                | Dropout                 | 0     \n",
      "43  | output                                               | Conv1d                  | 363   \n",
      "44  | ma_aoa_lin1                                          | Linear                  | 48    \n",
      "45  | ma_aoa_relu1                                         | ReLU                    | 0     \n",
      "46  | ma_aoa_lin2                                          | Linear                  | 544   \n",
      "47  | ma_aoa_relu2                                         | ReLU                    | 0     \n",
      "48  | latenc_transformer                                   | TransformerDecoder      | 25.7 K\n",
      "49  | latenc_transformer.layers                            | ModuleList              | 25.7 K\n",
      "50  | latenc_transformer.layers.0                          | TransformerDecoderLayer | 12.8 K\n",
      "51  | latenc_transformer.layers.0.self_attn                | MultiheadAttention      | 4.2 K \n",
      "52  | latenc_transformer.layers.0.self_attn.out_proj       | _LinearWithBias         | 1.1 K \n",
      "53  | latenc_transformer.layers.0.multihead_attn           | MultiheadAttention      | 4.2 K \n",
      "54  | latenc_transformer.layers.0.multihead_attn.out_proj  | _LinearWithBias         | 1.1 K \n",
      "55  | latenc_transformer.layers.0.linear1                  | Linear                  | 2.1 K \n",
      "56  | latenc_transformer.layers.0.dropout                  | Dropout                 | 0     \n",
      "57  | latenc_transformer.layers.0.linear2                  | Linear                  | 2.1 K \n",
      "58  | latenc_transformer.layers.0.norm1                    | LayerNorm               | 64    \n",
      "59  | latenc_transformer.layers.0.norm2                    | LayerNorm               | 64    \n",
      "60  | latenc_transformer.layers.0.norm3                    | LayerNorm               | 64    \n",
      "61  | latenc_transformer.layers.0.dropout1                 | Dropout                 | 0     \n",
      "62  | latenc_transformer.layers.0.dropout2                 | Dropout                 | 0     \n",
      "63  | latenc_transformer.layers.0.dropout3                 | Dropout                 | 0     \n",
      "64  | latenc_transformer.layers.1                          | TransformerDecoderLayer | 12.8 K\n",
      "65  | latenc_transformer.layers.1.self_attn                | MultiheadAttention      | 4.2 K \n",
      "66  | latenc_transformer.layers.1.self_attn.out_proj       | _LinearWithBias         | 1.1 K \n",
      "67  | latenc_transformer.layers.1.multihead_attn           | MultiheadAttention      | 4.2 K \n",
      "68  | latenc_transformer.layers.1.multihead_attn.out_proj  | _LinearWithBias         | 1.1 K \n",
      "69  | latenc_transformer.layers.1.linear1                  | Linear                  | 2.1 K \n",
      "70  | latenc_transformer.layers.1.dropout                  | Dropout                 | 0     \n",
      "71  | latenc_transformer.layers.1.linear2                  | Linear                  | 2.1 K \n",
      "72  | latenc_transformer.layers.1.norm1                    | LayerNorm               | 64    \n",
      "73  | latenc_transformer.layers.1.norm2                    | LayerNorm               | 64    \n",
      "74  | latenc_transformer.layers.1.norm3                    | LayerNorm               | 64    \n",
      "75  | latenc_transformer.layers.1.dropout1                 | Dropout                 | 0     \n",
      "76  | latenc_transformer.layers.1.dropout2                 | Dropout                 | 0     \n",
      "77  | latenc_transformer.layers.1.dropout3                 | Dropout                 | 0     \n",
      "78  | transformer_decoder                                  | TransformerDecoder      | 38.5 K\n",
      "79  | transformer_decoder.layers                           | ModuleList              | 38.5 K\n",
      "80  | transformer_decoder.layers.0                         | TransformerDecoderLayer | 12.8 K\n",
      "81  | transformer_decoder.layers.0.self_attn               | MultiheadAttention      | 4.2 K \n",
      "82  | transformer_decoder.layers.0.self_attn.out_proj      | _LinearWithBias         | 1.1 K \n",
      "83  | transformer_decoder.layers.0.multihead_attn          | MultiheadAttention      | 4.2 K \n",
      "84  | transformer_decoder.layers.0.multihead_attn.out_proj | _LinearWithBias         | 1.1 K \n",
      "85  | transformer_decoder.layers.0.linear1                 | Linear                  | 2.1 K \n",
      "86  | transformer_decoder.layers.0.dropout                 | Dropout                 | 0     \n",
      "87  | transformer_decoder.layers.0.linear2                 | Linear                  | 2.1 K \n",
      "88  | transformer_decoder.layers.0.norm1                   | LayerNorm               | 64    \n",
      "89  | transformer_decoder.layers.0.norm2                   | LayerNorm               | 64    \n",
      "90  | transformer_decoder.layers.0.norm3                   | LayerNorm               | 64    \n",
      "91  | transformer_decoder.layers.0.dropout1                | Dropout                 | 0     \n",
      "92  | transformer_decoder.layers.0.dropout2                | Dropout                 | 0     \n",
      "93  | transformer_decoder.layers.0.dropout3                | Dropout                 | 0     \n",
      "94  | transformer_decoder.layers.1                         | TransformerDecoderLayer | 12.8 K\n",
      "95  | transformer_decoder.layers.1.self_attn               | MultiheadAttention      | 4.2 K \n",
      "96  | transformer_decoder.layers.1.self_attn.out_proj      | _LinearWithBias         | 1.1 K \n",
      "97  | transformer_decoder.layers.1.multihead_attn          | MultiheadAttention      | 4.2 K \n",
      "98  | transformer_decoder.layers.1.multihead_attn.out_proj | _LinearWithBias         | 1.1 K \n",
      "99  | transformer_decoder.layers.1.linear1                 | Linear                  | 2.1 K \n",
      "100 | transformer_decoder.layers.1.dropout                 | Dropout                 | 0     \n",
      "101 | transformer_decoder.layers.1.linear2                 | Linear                  | 2.1 K \n",
      "102 | transformer_decoder.layers.1.norm1                   | LayerNorm               | 64    \n",
      "103 | transformer_decoder.layers.1.norm2                   | LayerNorm               | 64    \n",
      "104 | transformer_decoder.layers.1.norm3                   | LayerNorm               | 64    \n",
      "105 | transformer_decoder.layers.1.dropout1                | Dropout                 | 0     \n",
      "106 | transformer_decoder.layers.1.dropout2                | Dropout                 | 0     \n",
      "107 | transformer_decoder.layers.1.dropout3                | Dropout                 | 0     \n",
      "108 | transformer_decoder.layers.2                         | TransformerDecoderLayer | 12.8 K\n",
      "109 | transformer_decoder.layers.2.self_attn               | MultiheadAttention      | 4.2 K \n",
      "110 | transformer_decoder.layers.2.self_attn.out_proj      | _LinearWithBias         | 1.1 K \n",
      "111 | transformer_decoder.layers.2.multihead_attn          | MultiheadAttention      | 4.2 K \n",
      "112 | transformer_decoder.layers.2.multihead_attn.out_proj | _LinearWithBias         | 1.1 K \n",
      "113 | transformer_decoder.layers.2.linear1                 | Linear                  | 2.1 K \n",
      "114 | transformer_decoder.layers.2.dropout                 | Dropout                 | 0     \n",
      "115 | transformer_decoder.layers.2.linear2                 | Linear                  | 2.1 K \n",
      "116 | transformer_decoder.layers.2.norm1                   | LayerNorm               | 64    \n",
      "117 | transformer_decoder.layers.2.norm2                   | LayerNorm               | 64    \n",
      "118 | transformer_decoder.layers.2.norm3                   | LayerNorm               | 64    \n",
      "119 | transformer_decoder.layers.2.dropout1                | Dropout                 | 0     \n",
      "120 | transformer_decoder.layers.2.dropout2                | Dropout                 | 0     \n",
      "121 | transformer_decoder.layers.2.dropout3                | Dropout                 | 0     \n",
      "---------------------------------------------------------------------------------------------------\n",
      "97.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "97.4 K    Total params\n",
      "0.390     Total estimated model params size (MB)\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-45d4afebefac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# double dispatch to initiate the training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[1;31m# set stage for logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[1;34m(self, ref_model)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[1;31m# run eval step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_sanity_val_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;31m# allow no returns from eval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[1;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[0;32m    730\u001b[0m                 \u001b[1;31m# lightning module methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"evaluation_step_and_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m                     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"validation_step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"validation_step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;31m# capture any logged information\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\MA2\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-27ce42a8be58>\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[0my_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqoip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m         \u001b[0mxout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mxnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}